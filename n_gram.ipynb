{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUvPwHP584Ij"
      },
      "source": [
        "# Project: N-Gram Sentence Generator\n",
        "\n",
        "# Krishna Menon I045\n",
        "# Aditya Rana I059\n",
        "# Yash Kothari I037\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "**Objective:** To implement an n-gram-based language model (LM) capable of generating coherent sentences. This project involves preprocessing a text corpus, building bigram (n=2), trigram (n=3), and 4-gram (n=4) models with Laplace smoothing, and generating new sentences from a two-word prompt.\n",
        "\n",
        "**Evaluation:** The models will be evaluated quantitatively using **perplexity** on a held-out test set and qualitatively by analyzing the **fluency** and **coherence** of the generated sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwYboUAZ84Im",
        "outputId": "9b53ac04-ddd7-42cb-e7ee-b23ab91a0338"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported and NLTK data downloaded.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "print(\"Libraries imported and NLTK data downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vo9k-m5E84In",
        "outputId": "123c1421-0db7-40e0-ac2e-20a35ad6de12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sentences loaded: 7752\n",
            "Example sentence: ['But', 'James', 'will', 'not', 'like', 'to', 'put', 'the', 'horses', 'to', 'for', 'such', 'a', 'little', 'way', ';--', 'and', 'where', 'are', 'the', 'poor', 'horses', 'to', 'be', 'while', 'we', 'are', 'paying', 'our', 'visit', '?\"']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "raw_sentences = list(gutenberg.sents('austen-emma.txt'))\n",
        "\n",
        "print(f\"Total sentences loaded: {len(raw_sentences)}\")\n",
        "print(\"Example sentence:\", raw_sentences[50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thzd7HP984Io",
        "outputId": "ae736aa5-8ede-453f-fa22-cd15222502f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training sentences: 6201\n",
            "Testing sentences: 1551\n"
          ]
        }
      ],
      "source": [
        "split_index = int(0.8 * len(raw_sentences))\n",
        "train_raw_sents = raw_sentences[:split_index]\n",
        "test_raw_sents = raw_sentences[split_index:]\n",
        "\n",
        "print(f\"Training sentences: {len(train_raw_sents)}\")\n",
        "print(f\"Testing sentences: {len(test_raw_sents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LACPtQSb84Io",
        "outputId": "d59468c1-8f05-4550-cc57-5e6f8d185899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size (V): 6595\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(sentences):\n",
        "    vocab = set()\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            vocab.add(word.lower())\n",
        "    vocab.add('<UNK>')\n",
        "    vocab.add('<s>')\n",
        "    vocab.add('</s>')\n",
        "    return vocab\n",
        "\n",
        "vocabulary = build_vocab(train_raw_sents)\n",
        "V = len(vocabulary)\n",
        "print(f\"Vocabulary size (V): {V}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlA_74aJ84Ip",
        "outputId": "3cc2e08c-9309-46cb-a504-e3c1e36ffa04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example preprocessed sentence (train_data[50]):\n",
            "['<s>', '<s>', '<s>', 'but', 'james', 'will', 'not', 'like', 'to', 'put', 'the', 'horses', 'to', 'for', 'such', 'a', 'little', 'way', ';--', 'and', 'where', 'are', 'the', 'poor', 'horses', 'to', 'be', 'while', 'we', 'are', 'paying', 'our', 'visit', '?\"', '</s>']\n"
          ]
        }
      ],
      "source": [
        "def preprocess(sentences, vocab_set):\n",
        "    processed_sentences = []\n",
        "    n_pad = 3 #\n",
        "\n",
        "    for sent in sentences:\n",
        "        tokens = []\n",
        "        for word in sent:\n",
        "            word_low = word.lower()\n",
        "            if word_low in vocab_set:\n",
        "                tokens.append(word_low)\n",
        "            else:\n",
        "                tokens.append('<UNK>')\n",
        "        padding = [\"<s>\"] * n_pad\n",
        "        processed_sentences.append(padding + tokens + [\"</s>\"])\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "train_data = preprocess(train_raw_sents, vocabulary)\n",
        "test_data = preprocess(test_raw_sents, vocabulary)\n",
        "\n",
        "print(\"Example preprocessed sentence (train_data[50]):\")\n",
        "print(train_data[50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n-gl-nPp84Ip"
      },
      "outputs": [],
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, n, k, vocab):\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.vocab = vocab\n",
        "        self.V = len(vocab)\n",
        "        self.n_gram_counts = defaultdict(int)\n",
        "        self.prefix_counts = defaultdict(int)\n",
        "\n",
        "    def fit(self, sentences):\n",
        "        print(f\"Training {self.n}-gram model...\")\n",
        "        start_idx = 3 - (self.n - 1)\n",
        "        for sentence in sentences:\n",
        "            tokens = sentence[start_idx:]\n",
        "            for i in range(len(tokens) - self.n + 1):\n",
        "                n_gram = tuple(tokens[i : i + self.n])\n",
        "                prefix = tuple(tokens[i : i + self.n - 1])\n",
        "                self.n_gram_counts[n_gram] += 1\n",
        "                self.prefix_counts[prefix] += 1\n",
        "        print(\"Training complete.\")\n",
        "\n",
        "    def get_prob(self, n_gram):\n",
        "        \"\"\"Calculates the smoothed probability of a single n-gram.\"\"\"\n",
        "        prefix = n_gram[:-1]\n",
        "        numerator = self.n_gram_counts[n_gram] + self.k\n",
        "        denominator = self.prefix_counts[prefix] + (self.k * self.V)\n",
        "        if denominator == 0:\n",
        "            return 1 / self.V\n",
        "\n",
        "        return numerator / denominator\n",
        "\n",
        "    def get_next_word_probs(self, prefix_words):\n",
        "        \"\"\"Returns a probability distribution for all words in vocab given a prefix.\"\"\"\n",
        "        prefix = tuple(prefix_words[-(self.n-1):])\n",
        "        probs = {}\n",
        "        denominator = self.prefix_counts[prefix] + (self.k * self.V)\n",
        "        for word in self.vocab:\n",
        "            n_gram = prefix + (word,)\n",
        "            numerator = self.n_gram_counts[n_gram] + self.k\n",
        "            probs[word] = numerator / denominator\n",
        "\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBsujvym84Iq",
        "outputId": "96a26325-e587-44a8-ce84-719b51766816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training 2-gram model...\n",
            "Training complete.\n",
            "Training 3-gram model...\n",
            "Training complete.\n",
            "Training 4-gram model...\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "k_val = 1\n",
        "\n",
        "model_2 = NGramModel(n=2, k=k_val, vocab=vocabulary)\n",
        "model_2.fit(train_data)\n",
        "\n",
        "model_3 = NGramModel(n=3, k=k_val, vocab=vocabulary)\n",
        "model_3.fit(train_data)\n",
        "\n",
        "model_4 = NGramModel(n=4, k=k_val, vocab=vocabulary)\n",
        "model_4.fit(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rnhRVYdf84Iq"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(model, start_words, max_len=12):\n",
        "    n = model.n\n",
        "    sentence = [w.lower() for w in start_words]\n",
        "    context = ([\"<s>\"] * 3) + [w.lower() for w in start_words]\n",
        "    for _ in range(max_len - len(start_words)):\n",
        "        prefix = tuple(context[-(n-1):])\n",
        "        word_probs = model.get_next_word_probs(prefix)\n",
        "        words = list(word_probs.keys())\n",
        "        probs = list(word_probs.values())\n",
        "        probs_sum = sum(probs)\n",
        "        normalized_probs = [p / probs_sum for p in probs]\n",
        "        next_word = np.random.choice(words, p=normalized_probs)\n",
        "        if next_word == \"</s>\":\n",
        "            break\n",
        "        sentence.append(next_word)\n",
        "        context.append(next_word)\n",
        "\n",
        "    return \" \".join(sentence) + \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCbGN59J84Iq",
        "outputId": "6e079935-cfe7-481b-abbd-b2175721a74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Bigram Model (n=2) \n",
            "1: once upon lame wherever walk caprice bounds cellery apprehensive throat bad whenever.\n",
            "2: once upon fellow miss regulate unexceptionably grown pitcher regards pursued bangs doubled.\n",
            "3: once upon stilton trimming _times_ respective chief accompanying comprehend undistinguishing peculiar inconsiderable.\n",
            "4: once upon haunting maintained wakefield replies wholly next reproof recommendations uninterruptedly horror.\n",
            "5: once upon mount effectually cramp occurred spreading knife nearer check answering ?).\n",
            "\n",
            " Trigram Model (n=3)  \n",
            "1: once upon _way_ spurn entrapped temper owner right degradation opportunity various propensity.\n",
            "2: once upon bella mystery averted errand supposed unmanageable tear marked regular imagines.\n",
            "3: once upon boy cox disturb appears undertaking staid awes forestalling cheerfuller apiece.\n",
            "4: once upon disturbance envy passionately ardent throws ?-- hurt best little chained.\n",
            "5: once upon proverb state washed partridge work seven painful formal pales acknowledgment.\n",
            "\n",
            " 4-gram Model (n=4)  \n",
            "1: once upon buyings helpmate reply takes wholesomeness alliances accompany covering leather quitted.\n",
            "2: once upon _her_ musician overcoming works adoption _we_ easier broadway road composedly.\n",
            "3: once upon allowances condition condemn confers antidote begins engagements yes kitty baked.\n",
            "4: once upon striving coffee itself enjoyed examining inebriety inquiring imagination oddest cut.\n",
            "5: once upon shoulder love widow subduing referring candour compressed wondered clerks importation.\n"
          ]
        }
      ],
      "source": [
        "start_prompt = [\"Once\", \"upon\"]\n",
        "num_sentences = 5\n",
        "\n",
        "print(\" Bigram Model (n=2) \")\n",
        "for i in range(num_sentences):\n",
        "    print(f\"{i+1}: {generate_sentence(model_2, start_prompt)}\")\n",
        "\n",
        "print(\"\\n Trigram Model (n=3)  \")\n",
        "for i in range(num_sentences):\n",
        "    print(f\"{i+1}: {generate_sentence(model_3, start_prompt)}\")\n",
        "\n",
        "print(\"\\n 4-gram Model (n=4)  \")\n",
        "for i in range(num_sentences):\n",
        "    print(f\"{i+1}: {generate_sentence(model_4, start_prompt)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "In8sTOnu84It"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(model, test_sentences):\n",
        "    print(f\"Calculating perplexity for n={model.n}...\")\n",
        "    n = model.n\n",
        "    total_log_prob = 0\n",
        "    total_words = 0\n",
        "    start_idx = 3 - (n - 1)\n",
        "    for sentence in test_sentences:\n",
        "        tokens = sentence[start_idx:]\n",
        "        total_words += len(tokens) - (n - 1)\n",
        "        for i in range(n - 1, len(tokens)):\n",
        "            n_gram = tuple(tokens[i - n + 1 : i + 1])\n",
        "            prob = model.get_prob(n_gram)\n",
        "            total_log_prob += np.log2(prob)\n",
        "\n",
        "\n",
        "    avg_log_likelihood = total_log_prob / total_words\n",
        "    perplexity = np.power(2, -avg_log_likelihood)\n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2olt0A4k84Iv",
        "outputId": "6c4db2d8-3d37-45f5-f437-02e6a2dd2cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating perplexity for n=2...\n",
            "Calculating perplexity for n=3...\n",
            "Calculating perplexity for n=4...\n",
            "| Model         |   Perplexity (on Test Set) |\n",
            "|:--------------|---------------------------:|\n",
            "| Bigram (n=2)  |                     738.85 |\n",
            "| Trigram (n=3) |                    3156.12 |\n",
            "| 4-gram (n=4)  |                    4707.38 |\n"
          ]
        }
      ],
      "source": [
        "ppl_2 = calculate_perplexity(model_2, test_data)\n",
        "ppl_3 = calculate_perplexity(model_3, test_data)\n",
        "ppl_4 = calculate_perplexity(model_4, test_data)\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\"Bigram (n=2)\", \"Trigram (n=3)\", \"4-gram (n=4)\"],\n",
        "    \"Perplexity (on Test Set)\": [ppl_2, ppl_3, ppl_4]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.set_index(\"Model\", inplace=True)\n",
        "print(results_df.to_markdown(floatfmt=\".2f\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCWf7PB84Iv"
      },
      "source": [
        "## **Quantitative (Perplexity):**\n",
        " We expect perplexity to decrease as n increases. The 4-gram model should have the lowest perplexity, followed by the trigram, and then the bigram. This is because models with larger n can use more context to predict the next word, making them more accurate. The 4-gram model's predictions P(w | w-3, w-2, w-1) are much more informed than the bigram's  P(w | w-1).\n",
        "\n",
        "\n",
        "## **Qualitative (Fluency):**\n",
        "In our assignment, we learned that sentences generated by our model corresponded with the perplexity scores that we were observing. The result of the bigram model was gibberish. The trigram form was comprised of a skeleton of good grammar, and yet of no actual connection.\n",
        "\n",
        "\n",
        "## **Limitations of N-Gram Models:**\n",
        " When we pushed up to 4-gram, the phrases became the most fluent and locally coherent which really was the best of them being able to use a larger context window. The possible n-grams increase exponentially (Vn). The majority of 4 grams such as once upon a time never appear in our training corpus even when it is massive. That means the count is zero. A non-zero probability will be obtained with Laplace smoothing, but it is more of a conjecture. The reason is that, with large n (larger than 5 or 6) you will find perplexity again rising, because at this point the n-gram model will no longer be able to remember the long-term context, which is the entire conversation. A 4 gram cannot grasp what was being spoken 10 words ago. That is why it is unable to produce really coherent paragraph. It does not pick semantics and dependencies. To give a few examples, â€œThe man who lives down the street... is... may drop the subject man and suppose that it is speaking about the street.\n",
        "\n",
        "\n",
        "## **Conclusion:**\n",
        " This project has managed to demonstrate the construction of n-gram language models successfully. We had a definite trade-off: increasing n to 4 resulted in better quality (less perplexity, higher fluency), because more context was captured, but deteriorated sparsity and computational cost. The experiment introduces the major drawbacks of n-gram models and is a step towards more sophisticated methods, such as neural language models (RNNs, LSTMs, Transformers), which are capable of representing long-range interactions, as well as acquiring distributed representations of words."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
